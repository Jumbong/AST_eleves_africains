[
  {
    "objectID": "index_gdr.html",
    "href": "index_gdr.html",
    "title": "Spearman Correlation Matrix",
    "section": "",
    "text": "Compte tenu de ma spécialisation en gestion des risques, les contenus que je prévois de partager dans cette section dédiée à la 3ème année (3A) porteront principalement sur les enseignements spécifiques à cette spécialisation. Je m’efforcerai de rendre ces partages aussi pertinents et enrichissants que possible, en espérant qu’ils serviront de guide précieux pour ceux qui suivront une voie similaire."
  },
  {
    "objectID": "index_gdr.html#the-prediction-risk",
    "href": "index_gdr.html#the-prediction-risk",
    "title": "Spearman Correlation Matrix",
    "section": "The prediction risk",
    "text": "The prediction risk\nThe risk prediction is the expected loss between the predicted value and the true value on new, unseen data. In formula form, it can be expressed as: \\[\n\\text{R(X)} = \\mathbb{E}[(Y - \\hat{Y})^2] = \\mathbb{E}[(Y - \\mathbb{text{E}}(Y|X))^2]\n\\] where \\(Y\\) is the true value, \\(\\hat{Y}\\) is the predicted value, and \\(X\\) is the set of covariates used for prediction.\n\nMay 28, 2025"
  },
  {
    "objectID": "index_gdr.html#observed-default-rate-odr",
    "href": "index_gdr.html#observed-default-rate-odr",
    "title": "Spearman Correlation Matrix",
    "section": "Observed Default Rate (ODR)",
    "text": "Observed Default Rate (ODR)\nHow can we estimate the observed default rate (ODR) of a portfolio of loans?\nThis is example of a database where we want to estimate the ODR and the yearly ODR as the the average of the quarterly ODRs.\n\nimport pandas as pd\nimport numpy as np\n\n\n# Étape 1 : Création de la table \"frequentist_PD\"\n# Simulons 6 trimestres de données pour 2 clusters (A et B)\nclusters = ['A'] * 6 + ['B'] * 6\ndates = pd.date_range('2020-01-01', periods=6, freq='QE').tolist() * 2\nodrQ = [0.020, 0.025, 0.030, 0.028, 0.027, 0.029,\n        0.030, 0.032, 0.031, 0.033, 0.035, 0.034]\n\nfrequentist_PD = pd.DataFrame({\n    'cluster': clusters,\n    'hpm_arret': dates,\n    'odrQ': odrQ\n})\n\n# On ajoute des colonnes d'index pour simuler \"obs\"\nfrequentist_PD['obs'] = frequentist_PD.groupby('cluster').cumcount()\n\n# On trie pour garantir l'ordre temporel par cluster\nfrequentist_PD = frequentist_PD.sort_values(['cluster', 'hpm_arret']).reset_index(drop=True)\n\nfrequentist_PD\n\n\n\n\n\n\n\n\ncluster\nhpm_arret\nodrQ\nobs\n\n\n\n\n0\nA\n2020-03-31\n0.020\n0\n\n\n1\nA\n2020-06-30\n0.025\n1\n\n\n2\nA\n2020-09-30\n0.030\n2\n\n\n3\nA\n2020-12-31\n0.028\n3\n\n\n4\nA\n2021-03-31\n0.027\n4\n\n\n5\nA\n2021-06-30\n0.029\n5\n\n\n6\nB\n2020-03-31\n0.030\n0\n\n\n7\nB\n2020-06-30\n0.032\n1\n\n\n8\nB\n2020-09-30\n0.031\n2\n\n\n9\nB\n2020-12-31\n0.033\n3\n\n\n10\nB\n2021-03-31\n0.035\n4\n\n\n11\nB\n2021-06-30\n0.034\n5\n\n\n\n\n\n\n\n\n# Étape 2 : Simuler les frequentist_PD_2_j (décalages pour j = i+1, i = 1, 2, 3)\n\n# On crée un dictionnaire pour stocker les décalages\nfrequentist_PD_2 = {}\n\n# On prépare une copie de base à laquelle on ajoutera les colonnes décalées\nbase = frequentist_PD.copy()\n\n# Pour chaque i (1 to 3), on génère le j = i + 1 et on décale les colonnes\nfor i in range(1, 4):\n    j = i + 1\n    shifted = base.copy()\n    shifted['obs'] = shifted['obs'] - i  # décale l'observation comme point=obs+i en SAS\n    shifted = shifted.rename(columns={\n        'cluster': f'cluster{j}',\n        'hpm_arret': f'hpm_arret{j}',\n        'odrQ': f'odrQ{j}'\n    })\n    frequentist_PD_2[j] = shifted[['obs', f'cluster{j}', f'hpm_arret{j}', f'odrQ{j}']]\n\n# On merge progressivement pour simuler les frequentist_PD_3_j\nmerged = base.copy()\nfor j in range(2, 5):\n    merged = pd.merge(merged, frequentist_PD_2[j], on='obs', how='left')\n\nmerged\n\n\n\n\n\n\n\n\ncluster\nhpm_arret\nodrQ\nobs\ncluster2\nhpm_arret2\nodrQ2\ncluster3\nhpm_arret3\nodrQ3\ncluster4\nhpm_arret4\nodrQ4\n\n\n\n\n0\nA\n2020-03-31\n0.020\n0\nA\n2020-06-30\n0.025\nA\n2020-09-30\n0.030\nA\n2020-12-31\n0.028\n\n\n1\nA\n2020-03-31\n0.020\n0\nA\n2020-06-30\n0.025\nA\n2020-09-30\n0.030\nB\n2020-12-31\n0.033\n\n\n2\nA\n2020-03-31\n0.020\n0\nA\n2020-06-30\n0.025\nB\n2020-09-30\n0.031\nA\n2020-12-31\n0.028\n\n\n3\nA\n2020-03-31\n0.020\n0\nA\n2020-06-30\n0.025\nB\n2020-09-30\n0.031\nB\n2020-12-31\n0.033\n\n\n4\nA\n2020-03-31\n0.020\n0\nB\n2020-06-30\n0.032\nA\n2020-09-30\n0.030\nA\n2020-12-31\n0.028\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n57\nB\n2020-12-31\n0.033\n3\nB\n2021-03-31\n0.035\nA\n2021-06-30\n0.029\nNaN\nNaT\nNaN\n\n\n58\nB\n2020-12-31\n0.033\n3\nB\n2021-03-31\n0.035\nB\n2021-06-30\n0.034\nNaN\nNaT\nNaN\n\n\n59\nB\n2021-03-31\n0.035\n4\nA\n2021-06-30\n0.029\nNaN\nNaT\nNaN\nNaN\nNaT\nNaN\n\n\n60\nB\n2021-03-31\n0.035\n4\nB\n2021-06-30\n0.034\nNaN\nNaT\nNaN\nNaN\nNaT\nNaN\n\n\n61\nB\n2021-06-30\n0.034\n5\nNaN\nNaT\nNaN\nNaN\nNaT\nNaN\nNaN\nNaT\nNaN\n\n\n\n\n62 rows × 13 columns\n\n\n\nThe SAS script is given below:\n%macro test1;\n\n  %do i=1 %to 3 %by 1;\n    %let j = %sysevalf(&i.+1);\n\n    /* Crée une table contenant les colonnes odrQ décalées */\n    data frequentist_PD_2_&j.;\n      obs1 = 1;\n      do while (obs1 &lt; nobs);\n        set frequentist_PD nobs=nobs;\n        obs&j. = obs1 + &i.;\n\n        set frequentist_PD (\n          rename=(\n            cluster=cluster&j.\n            lb=lb&j.\n            ub=ub&j.\n            hpm_arret=hpm_arret&j.\n            def=defQ&j.\n            n=nQ&j.\n            odrQ=odrQ&j.\n          )\n        ) point=obs&j.;\n\n        output;\n        keep cluster lb ub hpm_arret def defQ&j. n nQ&j. odrQ odrQ&j.;\n        obs1 + 1;\n      end;\n    run;\n\n    /* Initialisation au premier tour */\n    %if &i. = 1 %then %do;\n      data frequentist_PD_3_&j.;\n        set frequentist_PD_2_&j.;\n      run;\n    %end;\n\n    /* Jointure gauche avec la table précédente pour empiler les odrQ */\n    %else %do;\n      proc sql;\n        create table frequentist_PD_3_&j. as\n        select a.*,\n               b.defQ&j.,\n               b.nQ&j.,\n               b.odrQ&j.\n        from frequentist_PD_3_&i. as a\n        left join frequentist_PD_2_&j. as b\n          on a.cluster = b.cluster\n          and a.hpm_arret = b.hpm_arret;\n      quit;\n    %end;\n\n  %end;\n\n  /* Résultat final : table contenant odrQ, odrQ2, odrQ3, odrQ4 */\n  data frequentist_PD_4;\n    set frequentist_PD_3_&j.;\n    \n    /* Calcul du ODR annuel glissant comme moyenne des 4 trimestres */\n    odrY = mean(of odrQ odrQ2 odrQ3 odrQ4);\n  run;\n\n  /* Décale hpm_arret de 9 mois (équivalent intnx) */\n  data frequentist_PD_4;\n    set frequentist_PD_4;\n    hpm_arret = intnx('month', hpm_arret, 9, 'same');\n    if year(hpm_arret) = 2019 then delete; /* Exclusion des années trop anciennes */\n  run;\n\n  /* Sauvegarde finale */\n  data frequentist_PD_5;\n    set frequentist_PD_4;\n  run;\n\n%mend;\n\n/* Exécution de la macro */\n%test1;\n\nMay 27, 2025"
  },
  {
    "objectID": "index_gdr.html#model-selection",
    "href": "index_gdr.html#model-selection",
    "title": "Spearman Correlation Matrix",
    "section": "Model Selection",
    "text": "Model Selection\nWe have a data with many covariates. But we want to include only the covariates that are relevant to the response variable. This allows us to have a parsimonious model, with fewer covariates, which is easier to interpret and to use for prediction.\nGenerally, when we add more covariates to a model, the bias of the model decreases, but the variance increases.\n\nMay 26, 2025"
  },
  {
    "objectID": "index_gdr.html#the-adulterous-woman",
    "href": "index_gdr.html#the-adulterous-woman",
    "title": "Spearman Correlation Matrix",
    "section": "The adulterous woman",
    "text": "The adulterous woman\nA woman was caught in adultery. The Pharisees brought her to Jesus and asked him what should be done with her. They said that according the law of Moses, she should be stoned to death. Jesus replied, “Let anyone among you who is without sin be the first to throw a stone at her.”\nI like this story because it shows that Jesus is a feminist.\n\nMay 25, 2025"
  },
  {
    "objectID": "index_gdr.html#isaac-asimovs-three-laws-of-robotics",
    "href": "index_gdr.html#isaac-asimovs-three-laws-of-robotics",
    "title": "Spearman Correlation Matrix",
    "section": "Isaac Asimov’s “Three Laws of Robotics”",
    "text": "Isaac Asimov’s “Three Laws of Robotics”\n\nA robot may not injure a human being or, through inaction, allow a human being to come to harm.\nA robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\nA robot must protect its own existence as long as such protection does not conflict with the First or Second Law.\n\n\nMay 24, 2025"
  },
  {
    "objectID": "index_gdr.html#density-probability-function-using-plotly",
    "href": "index_gdr.html#density-probability-function-using-plotly",
    "title": "Spearman Correlation Matrix",
    "section": "Density probability function using plotly",
    "text": "Density probability function using plotly\n\nimport pandas as pd\nimport numpy as np\n\n# Simuler les données\ndf = pd.DataFrame({\n    '2012': np.random.randn(200),\n    '2013': np.random.randn(200) + 1\n})\n\n# Calculer les statistiques\nstats_df = pd.DataFrame({\n    'min': df.min(),\n    'mean': df.mean(),\n    'median': df.median(),\n    'max': df.max(),\n    'std': df.std()\n})\n\n# Affichage\nprint(stats_df)\n\n           min     mean    median       max       std\n2012 -2.399943  0.07926  0.075151  2.206767  0.880948\n2013 -1.076208  1.02229  0.963917  3.607347  0.920831\n\n\n\nimport plotly.express as px\nimport pandas as pd\nimport numpy as np\n\n# Simuler les données\ndf = pd.DataFrame({\n    '2012': np.random.randn(200),\n    '2013': np.random.randn(200) + 1\n})\n\n# Convertir au format long\ndf_long = df.melt(var_name='Year', value_name='Value')\n\n# Créer le boxplot\nfig = px.box(df_long, x='Year', y='Value', points=False, title=\"Boxplot for 2012 and 2013\", color='Year',\n             labels={'Year': 'Year', 'Value': 'Values'})\nfig.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom scipy.stats import gaussian_kde\n\n# Example data (you can replace this with your real df[\"age\"])\nage = np.array([15, 16, 16, 17, 18, 19, 20, 21, 22, 22, 23, 24, 25, 25, 26, 27, 28])\n\n# Remove NA if needed\nage = age[~np.isnan(age)]\n\n# KDE estimate\nkde = gaussian_kde(age)\n\n# Define range of x values\nx_vals = np.linspace(age.min(), age.max(), 200)\ny_vals = kde(x_vals)\n\n# Plot using Plotly\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x_vals, y=y_vals, mode='lines', name='Density', line=dict(width=2)))\nfig.update_layout(title=\"Density of age\", xaxis_title=\"Age\", yaxis_title=\"Density\")\nfig.show()"
  },
  {
    "objectID": "index_gdr.html#exploring-variable-relationships-in-python",
    "href": "index_gdr.html#exploring-variable-relationships-in-python",
    "title": "Spearman Correlation Matrix",
    "section": "Exploring Variable Relationships in Python",
    "text": "Exploring Variable Relationships in Python\nThe graphic analysis is a tool to understand the relationship between the covariates and the response variable. It is very important when we want to perform a linear regression.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.api import OLS, add_constant\n\n# Load the dataset\ndf = pd.read_csv('data/Multiple_Regression_Dataset.csv')\ndf.head()\n\n\n\n\n\n\n\n\nR\nAge\nS\nEd\nEx0\nEx1\nLF\nM\nN\nNW\nU1\nU2\nW\nX\n\n\n\n\n0\n79.1\n151\n1\n91\n58\n56\n510\n950\n33\n301\n108\n41\n394\n261\n\n\n1\n163.5\n143\n0\n113\n103\n95\n583\n1012\n13\n102\n96\n36\n557\n194\n\n\n2\n57.8\n142\n1\n89\n45\n44\n533\n969\n18\n219\n94\n33\n318\n250\n\n\n3\n196.9\n136\n0\n121\n149\n141\n577\n994\n157\n80\n102\n39\n673\n167\n\n\n4\n123.4\n141\n0\n121\n109\n101\n591\n985\n18\n30\n91\n20\n578\n174\n\n\n\n\n\n\n\n\n# Create a new figure\n\n# Extract response variable and covariates\nresponse = 'R'\ncovariates = [col for col in df.columns if col != response]\n\nfig, axes = plt.subplots(nrows=4, ncols=4, figsize=(5, 5), sharex=False, sharey=True)\naxes = axes.flatten()\n\n# Plot boxplot for binary variable 'S'\nsns.boxplot(data=df, x='S', y='R', ax=axes[0])\naxes[0].set_title('Boxplot of R by S')\naxes[0].set_xlabel('S')\naxes[0].set_ylabel('R')\n\n# Plot regression lines for all other covariates\nplot_index = 1\nfor cov in covariates:\n    if cov != 'S':\n        sns.regplot(data=df, x=cov, y='R', ax=axes[plot_index], scatter=True, line_kws={\"color\": \"red\"})\n        axes[plot_index].set_title(f'{cov} vs R')\n        axes[plot_index].set_xlabel(cov)\n        axes[plot_index].set_ylabel('R')\n        plot_index += 1\n\n# Hide unused subplots\nfor i in range(plot_index, len(axes)):\n    fig.delaxes(axes[i])\n\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "index_gdr.html#when-several-variables-are-correlated",
    "href": "index_gdr.html#when-several-variables-are-correlated",
    "title": "Spearman Correlation Matrix",
    "section": "When several variables are correlated",
    "text": "When several variables are correlated\nWhen several variables are correlated with each other, we keep only one of them. The one the most correlated with the response variable.\n\n# Step 2: Correlation of each variable with response R\nspearman_corr_with_R = spearman_corr['R'].drop('R')  # exclude R-R\n\n# Step 3: Identify pairs of covariates with strong inter-correlation (e.g., &gt; 0.9)\nstrong_pairs = []\nthreshold = 0.6\ncovariates = spearman_corr_with_R.index\n\nfor i, var1 in enumerate(covariates):\n    for var2 in covariates[i+1:]:\n        if abs(spearman_corr.loc[var1, var2]) &gt; threshold:\n            strong_pairs.append((var1, var2))\n\n# Step 4: From each correlated pair, keep only the variable most correlated with R\nto_keep = set()\nto_discard = set()\n\nfor var1, var2 in strong_pairs:\n    if abs(spearman_corr_with_R[var1]) &gt;= abs(spearman_corr_with_R[var2]):\n        to_keep.add(var1)\n        to_discard.add(var2)\n    else:\n        to_keep.add(var2)\n        to_discard.add(var1)\n\n# Final selection: all covariates excluding the ones to discard due to redundancy\nfinal_selected_variables = [var for var in covariates if var not in to_discard]\n\nfinal_selected_variables\n\n['Ex1', 'LF', 'M', 'N', 'NW', 'U2']"
  },
  {
    "objectID": "index_gdr.html#fit-a-linear-regression-model-on-a-six-variables-after-standardization-not-split-data-into-train-and-test",
    "href": "index_gdr.html#fit-a-linear-regression-model-on-a-six-variables-after-standardization-not-split-data-into-train-and-test",
    "title": "Spearman Correlation Matrix",
    "section": "Fit a linear regression model on a six variables after standardization not split data into train and test",
    "text": "Fit a linear regression model on a six variables after standardization not split data into train and test\n\n# Variables\nX = df[final_selected_variables]\ny = df['R']\n\n# Standardisation des variables explicatives\nscaler = StandardScaler()\nX_scaled_vars = scaler.fit_transform(X)\n\n# ➕ Remettre les noms des colonnes (après standardisation)\nX_scaled_df = pd.DataFrame(X_scaled_vars, columns=final_selected_variables)\n\n# ➕ Ajouter l'intercept (constante)\nX_scaled_df = add_constant(X_scaled_df)\n\n# Régression avec noms conservés\nmodel = OLS(y, X_scaled_df).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      R   R-squared:                       0.568\nModel:                            OLS   Adj. R-squared:                  0.503\nMethod:                 Least Squares   F-statistic:                     8.773\nDate:                Sat, 31 May 2025   Prob (F-statistic):           4.07e-06\nTime:                        16:51:47   Log-Likelihood:                -218.24\nNo. Observations:                  47   AIC:                             450.5\nDf Residuals:                      40   BIC:                             463.4\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         90.5085      3.975     22.767      0.000      82.474      98.543\nEx1           25.3077      5.008      5.054      0.000      15.187      35.429\nLF             4.9155      5.631      0.873      0.388      -6.465      16.296\nM              9.9027      5.681      1.743      0.089      -1.579      21.384\nN              2.6733      5.647      0.473      0.639      -8.741      14.087\nNW            11.1950      4.432      2.526      0.016       2.238      20.152\nU2             3.1268      4.928      0.634      0.529      -6.834      13.088\n==============================================================================\nOmnibus:                        1.619   Durbin-Watson:                   2.089\nProb(Omnibus):                  0.445   Jarque-Bera (JB):                1.126\nSkew:                           0.045   Prob(JB):                        0.569\nKurtosis:                       2.247   Cond. No.                         3.06\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "index_gdr.html#dont-trust-chatgpt",
    "href": "index_gdr.html#dont-trust-chatgpt",
    "title": "Spearman Correlation Matrix",
    "section": "Don’t Trust ChatGPT",
    "text": "Don’t Trust ChatGPT\nEach year, we see rapid advancements in artificial language tools, especially from companies like OpenAI. These models are evolving fast. Today, we can generate images with ease, solve complex problems in computer science, mathematics, physics, and more. But can we really trust these tools?\nI don’t think so.\nToday, I was writing an article about regression to the mean. The equation is given by:\n\n\\(\\mathbb{E}(Y|X) = \\alpha + \\beta X\\)\n\nMy goal was to explain the meaning of the regression coefficient and offer an intuitive technique to predict the value of \\(Y\\) given a value of \\(X\\). I assumed that the variances of \\(X\\) and \\(Y\\) were both equal to one, and that \\(X\\) was centered (i.e., had mean zero). I then stated that under these conditions, the slope \\(\\beta\\) is equal to the correlation between \\(X\\) and \\(Y\\).\nBut ChatGPT told me this was incorrect. It insisted that this only holds true if \\(Y\\) is also centered.\nYet, we know that if both \\(X\\) and \\(Y\\) are scaled to have unit variance, then the slope of the regression is indeed equal to the correlation between them—even if \\(Y\\) is not centered. That’s a basic identity in linear regression under standardized variables.\nSo while these AI tools can be helpful, they’re not always right. You still need to think critically, check the math, and trust your own understanding.\n\nMay 11, 2025"
  },
  {
    "objectID": "index_gdr.html#sports-bring-us-together",
    "href": "index_gdr.html#sports-bring-us-together",
    "title": "Spearman Correlation Matrix",
    "section": "Sports Bring Us Together",
    "text": "Sports Bring Us Together\nToday, like most days, I went for a run and stopped at the park to do some exercises—push-ups, a few pull-ups, and some jump rope.\nEvery time, I find it amazing. I meet people from all walks of life—different backgrounds, all genders, all ages. Some look wealthy, others look like they’re struggling. Some seem strong and healthy, others clearly carry the weight of illness. There are professionals and amateurs. Some are trying sports for the very first time—and probably won’t be back. Others stick with it for a while before giving up. And then, of course, there are the regulars who keep showing up.\nBut what stays constant is the atmosphere. It’s always welcoming. When you greet someone, they greet you back—with kindness. That simple act of saying hello when you arrive creates a sense of safety. You never know what might happen out there, but that greeting—it’s a small sign of trust, of connection. It reminds me of Marcel Mauss’s idea of the gift and counter-gift: you offer a smile, and in return, you receive not just a smile, but a sense of protection.\n\nMay 11, 2025"
  },
  {
    "objectID": "index_gdr.html#why-certain-couples-marry-and-others-do-not",
    "href": "index_gdr.html#why-certain-couples-marry-and-others-do-not",
    "title": "Spearman Correlation Matrix",
    "section": "Why certain couples marry and others do not?",
    "text": "Why certain couples marry and others do not?\nToday, saturday, May 10 2025, I attended the wedding of a friend–a girl I met in high school. We were in the same grade from ninth grade all the way through senior year.\nShe was smart, almost among the top students in our class. She was attentive, displined, serious and hardworking. She rarely laughted out loud, but she always had a warm smile. Given her academic achievements and her work ethic, I expected her to pursue advanced studies and build a successful career. And that’s exactly what happened–she went on to study at ENSAE, one of the most prestigious schools in France, and she is now an actuary.\nNow she is married too, and I hope she will find the same success in her marriage as she did in her studies and career. But this made me wonder: why do some people marry and others do not? What are the factors that influence the decision to marry?\nAt her wedding, I began to reflect on this question using the story of my friend as a starting point. First, she and her husband come from similar social backgrounds—which I believe played a stabilizing role in their relationship.\nThen, there was distance. After her master’s, she earned a scholarship to continue her studies at ENSAE and moved to Paris, while her husband remained in Cameroon. They had to maintain a long-distance relationship for two years and waited until the third year to get married. Despite the distance, they managed to stay connected and committed.\nLastly, their social circle made a real difference. The groom repeatedly thanked his friends during the ceremony for supporting their relationship. He said they reassured his bride during moments of doubt and helped keep them grounded and hopeful.\nBy the end of the ceremony, I had identified three key factors that seemed to have supported their union: shared social status, the ability to overcome physical distance, and the strength of their support network. I believe there are many other factors yet to discover, and I’m curious to explore what else might influence the decision to marry.\n\nMay 10, 2025"
  },
  {
    "objectID": "index_gdr.html#regression-to-the-mean",
    "href": "index_gdr.html#regression-to-the-mean",
    "title": "Spearman Correlation Matrix",
    "section": "Regression to the mean",
    "text": "Regression to the mean\nRegression to the mean was discovered and named by Sir Francis Galton in the 19th century. It refers to the phenomenon where extreme observations tend to be followed by more moderate ones, and moving closer to the average.\nThis concept is often misunderstood, and interpreted in terms of causality. Take this proposition: &gt; “Highly intelligent women tend to marry men who are less intelligent than they are.” What is the explanation?\nSome may think of highly intelligent women wanting to avoid the competition equally intelligent men, or being forced to compromise in their choice of partner because intelligent men do not want to compete with intelligent women. This explanation is wrong.\nThe correlation between the intelligence scores of spouces is less than perfect. If the correlation between the intelligence scores of spouses is not perfect( and if men and women on average do not differ in intelligence), then it is mathematically inevitable that the higly intelligent women will be married to men who are on overage less intelligent than they are.\n\nMay 9, 2025"
  },
  {
    "objectID": "index_gdr.html#should-you-invest-save-or-keep-your-money-under-the-mattress",
    "href": "index_gdr.html#should-you-invest-save-or-keep-your-money-under-the-mattress",
    "title": "Spearman Correlation Matrix",
    "section": "Should You Invest, Save, or Keep Your Money Under the Mattress?",
    "text": "Should You Invest, Save, or Keep Your Money Under the Mattress?\nA quick read of the bible can help you to answer this question. I recommend reading Mathieu 25:14, the parable of the servants and the master’s reward.\nIn this parable, a master gives three servants different amounts of money to manage during his absence. The first servant receives five talents, the second two talents, and the third one talent. The first two servants invest their money and double it, while the third servant hides his talent in the ground.\nWhen the master returned from his journey, the first two servants presented their profits. The master was proud of them and rewarded them. But the third servant explained he knew that his master was hard and demanding. Because he was afraid of losing the money, he hid the talent in the ground. The master was angry and disappointed, and he took the talent. He told him that if he was so afraid, the least he could have done was put the money in a bank to earn interest.\nThis parable teaches us that the most important to get money is to invest it wisely. If you are afraid of losing your money, you should at least put it in a bank to earn interest. Keep money under a pillow is not a good idea, and should be used as the last resort.\n\nMay 8, 2025"
  },
  {
    "objectID": "index_gdr.html#how-can-we-choose-to-modeling-time-series-using-ardl",
    "href": "index_gdr.html#how-can-we-choose-to-modeling-time-series-using-ardl",
    "title": "Spearman Correlation Matrix",
    "section": "How can we choose to modeling time series using ARDL?",
    "text": "How can we choose to modeling time series using ARDL?\nWhat ultimately determines the choice of a model is the data. This assumes that we’ve already carried out a preliminary analysis to understand the nature of the risk, define its scope, and identify the relevant risk factors.\nThat’s why it’s misguided to say, for example, that we should—or should not—use an ARDL model to analyze a time series. What truly matters is ensuring that the residuals of the model are not autocorrelated.\n\nMay 7, 2025"
  },
  {
    "objectID": "index_gdr.html#what-drives-stock-prices",
    "href": "index_gdr.html#what-drives-stock-prices",
    "title": "Spearman Correlation Matrix",
    "section": "What drives stock prices?",
    "text": "What drives stock prices?\nIt’s been about a week since I invested in Eutelsat stock at €3.59 through my PEA (Plan d’Épargne en Actions). When I woke up on Monday, May 5th, I was surprised to see that the stock had jumped to €4.20. Suddenly, it started dropping, and I was tempted to sell. Fortunately, I didn’t—because by the end of the day, the stock had climbed to over €4.65.\nThat’s a 29.5% increase relative to my investment:\n\\[\n\\frac{4.65 - 3.59}{3.59} \\times 100 ≈ 29.5\\%\n\\]\nNaturally, I started wondering what could have caused such a sharp rise. I looked it up on Google and checked the news on TV—but found nothing.\nLater that afternoon—maybe by chance, maybe because I had been actively searching—I stumbled upon an article in Les Échos. It mentioned that Eutelsat’s CEO, Eva Berneke, had been replaced by Jean-François Fallacher, the former CEO of Orange.\nThat was likely one reason for the stock’s spike. Another possible explanation is the French government’s involvement with the company. In the context of European and French national defense, Eutelsat is seen as a strategic alternative to Starlink, SpaceX’s satellite internet service.\nIt’s incredibly hard to predict stock prices.\nOn a similar note, I also noticed that oil prices had dropped recently. And again, there’s a potential explanation: the number of oil producers within OPEC has increased by three. This rise in oil production leads to a greater supply, which in turn explains the drop in oil prices.\n\nMay 6, 2025"
  },
  {
    "objectID": "index_gdr.html#we-are-champions-the-champions-the-first-trophy-of-harry-kane-in-his-career.",
    "href": "index_gdr.html#we-are-champions-the-champions-the-first-trophy-of-harry-kane-in-his-career.",
    "title": "Spearman Correlation Matrix",
    "section": "We are champions the champions : The first trophy of Harry Kane in his career.",
    "text": "We are champions the champions : The first trophy of Harry Kane in his career.\nYesterday, after Bayer Leverkusen’s draw, Bayern Munich officially clinched the Bundesliga title. This marks the very first trophy of Harry Kane’s career.\nOne question comes to mind: Is this title more meaningful to Harry Kane than it is to other Bayern players like Thomas Müller or Manuel Neuer, who’ve already won countless trophies?I want to take it even further: Is Kane happier about this title than a die-hard Bayern Munich fan might be? To answer those questions properly, we’d have to consider several factors—but let me just share my opinion.\nIt’s true that after a long, demanding season, winning a title is always a great source of satisfaction for any player. But for a club like Bayern Munich, winning the Bundesliga is important, yes—but it’s also expected. The fans and the club’s leadership invest a lot of money in top-tier talent to win the Champions League. So when you’re a Bayern player, lifting the Bundesliga trophy isn’t enough—you need to win the Champions League to feel truly fulfilled.\nNow, for someone like Harry Kane, who’s never won a single trophy in his career and who’s been through a lot of disappointment and heartbreak—with Tottenham and even with England—this title must feel incredibly rewarding. Let’s not forget: he lost the Champions League final in 2019 with Tottenham against Liverpool, and the Euro 2020 final with England against Italy. People even started saying he was cursed. Let’s hope he fully enjoys this title—and that it’s just the beginning of many more to come with Bayern Munich.\n\nMay 5, 2025"
  },
  {
    "objectID": "index_gdr.html#warren-buffett-retires-at-94",
    "href": "index_gdr.html#warren-buffett-retires-at-94",
    "title": "Spearman Correlation Matrix",
    "section": "Warren Buffett retires at 94",
    "text": "Warren Buffett retires at 94\nThe most famous investor in the world, Warren Buffett, has announced his retirement at the age of 94 in the end of the year. He has announced this decision during the annual meeting of Berkshire Hathaway, the company he founded in 1965.\nWhat explains his success? He is known for his long-term investment strategy, which focuses on buying and holding quality companies. He has also been a strong advocate of value investing, which involves looking for undervalued stocks with strong fundamentals.\nDo all these elements fully explain his success? I don’t think so. To explain his success, we also need to take luck into account. He wrote a book, and many others have written about him. But not many people have achieved the kind of success he has. That’s why I want to propose a model to predict Warren Buffett’s success, which will be formulated as follows:\nSuccess = f(strategy, long-term investment, value investing, …) + luck\n\nMay 4, 2025"
  },
  {
    "objectID": "index_gdr.html#diagnostic-de-performance-énergétique-dpe",
    "href": "index_gdr.html#diagnostic-de-performance-énergétique-dpe",
    "title": "Spearman Correlation Matrix",
    "section": "Diagnostic de performance énergétique (DPE)",
    "text": "Diagnostic de performance énergétique (DPE)\nLe DPE renseigne sur les performances énergétiques et environnementales d’un logement et d’un bâtiment, en évaluant ses émissions de gaz à effet de serre (GHG).\nLe DPE contient les informations suivantes : - Les informations sur les caractéristiques du bâtiment telles que la surface, les orientations, les mûrs, les fenêtres, les matériaux, etc. - Les informations sur les équipements du logement tels que le chauffage, la climatisation, l’eau chaude sanitaire, la ventilation, etc.\nLe contenu et les modalités du DPE sont réglementés. Ainsi, les données sur les DPE peuvent être utilisées comme facteur de risque ESG (environnemental, social et de gouvernance).\nPour plus d’informations, veuillez consulter le site de l’ademe ici\n\nMay 3, 2025"
  },
  {
    "objectID": "index_gdr.html#aimé-césaire-inspired-by-the-bible",
    "href": "index_gdr.html#aimé-césaire-inspired-by-the-bible",
    "title": "Spearman Correlation Matrix",
    "section": "Aimé Césaire inspired by the Bible",
    "text": "Aimé Césaire inspired by the Bible\nWhen we take time to reflect on the condition of the oppressed, the poor, and the suffering, we see that the Bible has inspired many well-known writers—among them, Aimé Césaire.\nIn Proverbs 31:8–9, it is written: “Speak up for those who cannot speak for themselves, for the rights of all who are abandoned.Speak up and judge fairly; defend the rights of the poor and needy.”\nWith this in mind, we can believe that Aimé Césaire—a powerful French poet—was deeply influenced by this text when he wrote: “My mouth will be the mouth of those who have no mouth, my voice, the freedom of those who sink into the dungeons of despair.”\n\nMay 2, 2025"
  },
  {
    "objectID": "index_gdr.html#la-clé-lamine-yamal",
    "href": "index_gdr.html#la-clé-lamine-yamal",
    "title": "Spearman Correlation Matrix",
    "section": "La clé Lamine Yamal",
    "text": "La clé Lamine Yamal\nHier soir, le barça affrontait l’Inter Milan dans le cadre du match aller de la demi-finale de la Ligue des champions. Ce match opposait la meilleure attaque de la compétition, le barça, à la meilleure défense, l’Inter Milan. On s’attendait donc à un match difficile et fermé avec peu de buts. Cependant, on a assisté à spectable incroyable, magnifique et inoubliable.\nL’Inter Milan a ouvert le score grâce à un but superbe de Marcus Thuram au tout début du match. L’inter de Milan a même doublé la mise grâce à un but de Dunfries. On s’est dit en ce moment que c’est fini pour le barça. L’Inter Milan a commencé à défendre, à mettre le bus. Mais il a fallu que Yamal trouve la clé pour ouvrir le cadenas mis en place par l’Inter.\nAprès avoir effacé Henrikh Mkhitaryan avec une facilité déconcertante, le joyau de la Masia a fixé Alessandro Bastoni puis décoché un remarquable tir du pied gauche qui est aller fracasser le poteau droit de Yann Sommer avant de franchir la ligne. On a vu en lui du Ronaldinho, du Neymar et même du Messi. On se pose même la question de savoir si c’est le nouveau Messi du Barça. Ce qui est sûr, c’est que Lamine Yamal est un génie, comme l’a dit son entraîneur, ancien entraîneur du Bayern Munich, Hans-Dieter Flick.\nIl est difficile de croire mais cet enfant sort de l’adolescence, mais il a une grande maturité. Il n’a que 17 ans et ce match est son 100e match avec le Barça. Il est le plus jeune joueur de l’histoire a avoir marqué en demi-finale de la Ligue des champions, dépassant ainsi le record d’un certain Kylian Mbappé.\nGrâce à lui, le Barça a pu arracher un match nul 3-3. Nous sommes impatients de voir Yamal briller lors du match retour à Milan."
  },
  {
    "objectID": "index_gdr.html#le-livre-des-réponses",
    "href": "index_gdr.html#le-livre-des-réponses",
    "title": "Spearman Correlation Matrix",
    "section": "Le livre des réponses",
    "text": "Le livre des réponses\nJ’adore la bible parce que tu peux y trouver des réponses à toutes tes questions. Par exemple, si tu est quelqu’un qui travaille beaucoup et dort peu, si ton entourage te dit qu’il faut dormir, que le sommeil est réparateur. Si tu es fatigué par eux, tu peux leur répondre que d’après Proverbes 20:13, “N’aime pas le sommeil, tu risquerais de t’appauvrir. Garde les yeux ouverts et tu seras rassasié de pain.”\nLire la bible, notamment le livre des proverbes ou de l’Ecclésiaste, écrit par Salomon, te donnera de l’intelligence et de la sagesse."
  },
  {
    "objectID": "index_gdr.html#categories",
    "href": "index_gdr.html#categories",
    "title": "Spearman Correlation Matrix",
    "section": "Categories",
    "text": "Categories\nHow tall is Junior? If Junior is 1.5m. Your answer is a function of his age. He is very tall if I tell you that he is 6 years old. Very short if he is 20 years old. Your brain automatically returns the relevant norm, which allows you to make a quick decision.\nWe are also able to match intensity across categories and answer the question: “How expensive is a restaurant that matches Junior’s height?”\nOur world is broken into categories for which we have a norm. And those norms allow us to make quick decisions."
  },
  {
    "objectID": "index_gdr.html#what-evokes-you-the-100-days-of-donald-trump",
    "href": "index_gdr.html#what-evokes-you-the-100-days-of-donald-trump",
    "title": "Spearman Correlation Matrix",
    "section": "What evokes you the 100 days of Donald Trump?",
    "text": "What evokes you the 100 days of Donald Trump?\nEven if Trump sticks to his program, the first 100 days have nonetheless been marked by a major disruption in the global economy, which has had a negative impact on various markets, including Wall Street, the European market, and the Asian market.\nFurthermore, we were shocked by his stances on the war in Ukraine, his attack on the Chairman of the Federal Reserve, Jerome Powell, as well as the increase in tariffs — especially in the context of the trade war with China.\n\nMay 1, 2025"
  },
  {
    "objectID": "index_gdr.html#la-puissance-de-la-parole.",
    "href": "index_gdr.html#la-puissance-de-la-parole.",
    "title": "Spearman Correlation Matrix",
    "section": "La puissance de la parole.",
    "text": "La puissance de la parole.\nLa bible met en évidence la puissance de la parole. Par la parole, Dieu à créé le monde. Pour des non croyants, ceci peut être ridicule. Je pense qu’ils conviennent avec moi que ceux qui maitrisent la parole ont un pouvoir. Ils peuvent séduire, enchanter, persuader, parfois ils peuvent même manipuler.\nDans la bible, notamment dans prophète, l’auteur préconise l’usage adéquat de la parole. Il dit que la langue, qui permet de parler, a un pouvoir de vie et de mort; ceux qui aiment parler en goûteront les fruits. Ensuite, il conseille de refléchir avant de parler. Celui qui répond avant d’avoir écouté fait preuve de folie et se couvre de honte.\nPlusieurs écrivaints ont souligné cette importance de la parole, notamment les mots. Jean Paul Sartre, un écrivain français, disait que “Les mots sont des pistolets chargés.”\n\nApril 30, 2025"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Jumbong Junior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "00_tds/model_selection.html",
    "href": "00_tds/model_selection.html",
    "title": "Model Selection",
    "section": "",
    "text": "Introduction\nWe will discuss the problem that may arise in multiple regression in practice. We have data with many variables but we may not want to include all of them in the model. We may not want to include all of them because some variables may be irrelevat, or because a smaller model with fewer variables has several advantages: It might give better predictions than a larger model and it is more parimonious [simple] then easier to interpret.In fact, as you add more variables, the bias of the model decreases, but the variance increases. This is known as the bias-variance trade-off. Too few variables yield high bias [this called underfitting].Too many covariates yields high variance [this called overfitting]. Good prediction requires a balance between bias and variance.\nMême après avoir utilisé une approche experte, [ou éliminer les rédondances entre les variables fortement corrélés, ou les variables qui ont un vif élévé dans le cadre de certains modèles de régressions] pour select relevant variables, il est possible que nous ayons encore trop de variables. A smaller model with fewer variables has several advantages: It might give better predictions than a larger model and it is more parimonious [simple] then easier to interpret. If we take the example of regression, as you add more variables, the bias of the model decreases, but the variance increases. This is known as the bias-variance trade-off. Too few variables yield high bias [this called underfitting].Too many covariates yields high variance [this called overfitting]. Good prediction requires a balance between bias and variance.\nC’est dans ce contexte que les mèthodes de sélection de variables sont utiles. There exists two main approaches to variable selection that we will explore : the first method consists to minimize the risk prediction and the second method assumes some subset of the \\(\\beta\\) coefficients are exactly equal to 0 and tries to find the true model, that is, the smallest sub-model consisting of nonzero \\(\\beta\\) terms.\n\n\n\n\n\n Back to topReferences\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in Statistical Inference. Springer Science & Business Media."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Meduim Publications",
    "section": "",
    "text": "This website serves as the treasury of my intellectual voyage, housing the jewels of my published articles on Medium, the budding seeds of works in progress, and the pillars of knowledge acquired during my third year in the Risk Management program at the National School of Statistics and Information Analysis in Rennes.\n\n\nMeduim Publications\nMedium is an online publishing platform for sharing ideas, stories, and knowledge through articles written by individuals and organizations.\n\nCredit Scoring modellling approach.\n\nDefinition of default and construction of the database.\nOutliers Identification and Treatment\nMissing data analysis\nDatabase splitting\nPre-selection of explanatory variables\nMethodological Approach To Merge Modalities of qualitative variables\nMethodological Approach To Discretize Quantitative variables\nMonotony and Stability\nOversampling\nLogistic Regression\n\nDefinition\nFitting The logistic Regression\nEvaluation of the logistic Regression\n\nAnalysis of the statistical significance of the model\nThe overall fit of the model : Likelihhod ratio test\nStatistical significance of individual regression coefficients: Likelihood ratio test and Z statistic\nHosmer-Lemeshow test\n\nContribution of variables\n\nModel performances\n\nAccuracy Ratio or Gini\nstability test\n\n\nTimes Series\nTowards data science (TDS) :\n\nProportional Odds Model for Ordinal Logistic Regression\nSpurious Regression in Time Series\nMultiple Linear Regression Analysis\nFourier Transformation\n\nOthers :\n\nSampling Bias and Class Imbalance in Maximum-likelihood Logistic Regression\nNumber of optimal defaults in credit rating\n\n\n\n\nLearning SAS\n\nBackground :\n\nPrerequisites\n\n\n\n\n\n\n Back to top"
  }
]